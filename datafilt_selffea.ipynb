{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33475, 25)\n",
      "33475\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_load(a):   # load data of one persons' 7 motion. a = '\\chx_'\n",
    "    ra = 'ra'\n",
    "    rd = 'rd'\n",
    "    sa = 'sa'\n",
    "    sd = 'sd'\n",
    "    sit = 'sit'\n",
    "    stand = 'stand'\n",
    "    walk = 'walk'\n",
    "\n",
    "    data_ra = sio.loadmat(r'D:\\code\\jupyter_file\\data'+a+ra)[ra]\n",
    "    data_rd = sio.loadmat(r'D:\\code\\jupyter_file\\data'+a+rd)[rd]\n",
    "    data_sa = sio.loadmat(r'D:\\code\\jupyter_file\\data'+a+sa)[sa]\n",
    "    data_sd = sio.loadmat(r'D:\\code\\jupyter_file\\data'+a+sd)[sd]\n",
    "    data_sit = sio.loadmat(r'D:\\code\\jupyter_file\\data'+a+sit)[sit]\n",
    "    data_stand = sio.loadmat(r'D:\\code\\jupyter_file\\data'+a+stand)[stand]\n",
    "    data_walk = sio.loadmat(r'D:\\code\\jupyter_file\\data'+a+walk)[walk]\n",
    "    return data_ra, data_rd, data_sa, data_sd, data_sit, data_stand, data_walk\n",
    "\n",
    "def filt_bad_data(data):\n",
    "    # count bad data number in 20-points window\n",
    "    index = []\n",
    "    for i in range(0,len(data) - 20, 20):\n",
    "        dataWin = data[i:i+20, 0:5]\n",
    "        if sum(sum(dataWin < 250)) < 2 and sum(sum(dataWin > 11000)) < 2:\n",
    "            index.append(i)\n",
    "\n",
    "    # process index[], connect data windows\n",
    "    index.append(99999)# add an inf to make final value right\n",
    "    a = index[0]\n",
    "    b = index[0] + 20\n",
    "    goodDataWinIndex = []\n",
    "    for i in range(0, len(index) - 1):\n",
    "        if b == index[i + 1]:\n",
    "            b = index[i + 1] + 20\n",
    "        else:\n",
    "            goodDataWinIndex.append([a, b])\n",
    "            a = index[i + 1]\n",
    "            b = index[i + 1] + 20\n",
    "\n",
    "    # filter the data with window length<80\n",
    "    row = np.size(goodDataWinIndex, 0)\n",
    "    col = np.size(goodDataWinIndex, 1)\n",
    "    finalIndex = []\n",
    "    for i in range(row):\n",
    "        if goodDataWinIndex[i][1] - goodDataWinIndex[i][0] >= 80:# locomotion lasting time over 1s\n",
    "            finalIndex.append(goodDataWinIndex[i])\n",
    "    return finalIndex\n",
    "\n",
    "def get_feature(data):  # calculate features\n",
    "    window_i = 20\n",
    "    window_l = 90\n",
    "    feature = np.zeros(((data.shape[1] - window_l)//window_i + 1, 25), dtype = float)\n",
    "    for i in range(5):\n",
    "        for j in range((data.shape[1] - window_l) // window_i):\n",
    "            feature[j, 5 * i] = np.mean(data[i, window_i*j : window_i*j + window_l])\n",
    "            feature[j, 5 * i + 1] = np.std(data[i, window_i*j : window_i*j + window_l])\n",
    "            feature[j, 5 * i + 2] = np.var(data[i, window_i*j : window_i*j + window_l])\n",
    "            feature[j, 5 * i + 3] = np.mean(abs(data[i, window_i*j : window_i*j + window_l] - np.mean(data[i, window_i*j : window_i*j + window_l])))\n",
    "            feature[j, 5 * i + 4] = max(data[i, window_i*j : window_i*j + window_l]) - min(data[i, window_i*j : window_i*j + window_l])\n",
    "    return feature\n",
    "\n",
    "# subject = ['chx_', 'hoy_', 'kl_', 'mm_', 'wyx_', 'yhl_', 'yjl_', 'yyj_', 'zjs_', 'zkj_', 'zs_']\n",
    "train_subject = ['\\chx_', '\\hoy_', '\\kl_', '\\mm_', '\\wyx_', '\\yhl_', '\\yjl_', '\\yyj_', '\\zjs_', '\\zkj_']\n",
    "data_ra = []\n",
    "data_rd = []\n",
    "data_sa = []\n",
    "data_sd = []\n",
    "data_sit = []\n",
    "data_stand = []\n",
    "data_walk = []\n",
    "\n",
    "for i in train_subject:\n",
    "    data = data_load(i)\n",
    "    data_ra.extend(data[0])\n",
    "    data_rd.extend(data[1])\n",
    "    data_sa.extend(data[2])\n",
    "    data_sd.extend(data[3])\n",
    "    data_sit.extend(data[4])\n",
    "    data_stand.extend(data[5])\n",
    "    data_walk.extend(data[6])\n",
    "\n",
    "data_ra = np.array(data_ra)\n",
    "data_rd = np.array(data_rd)\n",
    "data_sa = np.array(data_sa)\n",
    "data_sd = np.array(data_sd)\n",
    "data_sit = np.array(data_sit)\n",
    "data_stand = np.array(data_stand)\n",
    "data_walk = np.array(data_walk) # raw data, all data of the same motion\n",
    "\n",
    "#print(data_ra.shape)\n",
    "\n",
    "win_l = 90\n",
    "win_i = 20\n",
    "id_shift = 0\n",
    "\n",
    "ra_index = filt_bad_data(data_ra)\n",
    "rd_index = filt_bad_data(data_rd)\n",
    "sa_index = filt_bad_data(data_sa)\n",
    "sd_index = filt_bad_data(data_sd)\n",
    "walk_index = filt_bad_data(data_walk)\n",
    "stand_index = filt_bad_data(data_stand)\n",
    "sit_index = filt_bad_data(data_sit)\n",
    "\n",
    "data_id = []\n",
    "d1 = []\n",
    "d2 = []\n",
    "d3 = []\n",
    "d4 = []\n",
    "d5 = []\n",
    "y = []\n",
    "for a in ra_index:\n",
    "    for b in range((a[1] - a[0] - win_l + win_i) // win_i):\n",
    "        y.append(0)\n",
    "        data_id += [id_shift] * win_l\n",
    "        d1 += list(data_ra[a[0] + win_i * b : a[0] + win_i * b + win_l, 0])\n",
    "        d2 += list(data_ra[a[0] + win_i * b : a[0] + win_i * b + win_l, 1])\n",
    "        d3 += list(data_ra[a[0] + win_i * b : a[0] + win_i * b + win_l, 2])\n",
    "        d4 += list(data_ra[a[0] + win_i * b : a[0] + win_i * b + win_l, 3])\n",
    "        d5 += list(data_ra[a[0] + win_i * b : a[0] + win_i * b + win_l, 4])        \n",
    "        id_shift += 1\n",
    "filt_ra = np.array([d1, d2, d3, d4, d5])\n",
    "\n",
    "#print(len(data_id), len(d1))\n",
    "data_id = []\n",
    "d1 = []\n",
    "d2 = []\n",
    "d3 = []\n",
    "d4 = []\n",
    "d5 = []\n",
    "y = []\n",
    "for a in rd_index:\n",
    "    for b in range((a[1] - a[0] - win_l + win_i) // win_i):\n",
    "        y.append(1)\n",
    "        data_id += [id_shift] * win_l\n",
    "        d1 += list(data_rd[a[0] + win_i * b : a[0] + win_i * b + win_l, 0])\n",
    "        d2 += list(data_rd[a[0] + win_i * b : a[0] + win_i * b + win_l, 1])\n",
    "        d3 += list(data_rd[a[0] + win_i * b : a[0] + win_i * b + win_l, 2])\n",
    "        d4 += list(data_rd[a[0] + win_i * b : a[0] + win_i * b + win_l, 3])\n",
    "        d5 += list(data_rd[a[0] + win_i * b : a[0] + win_i * b + win_l, 4])        \n",
    "        id_shift += 1\n",
    "filt_rd = np.array([d1, d2, d3, d4, d5])\n",
    "        \n",
    "#print(len(data_id), len(d1))\n",
    "data_id = []\n",
    "d1 = []\n",
    "d2 = []\n",
    "d3 = []\n",
    "d4 = []\n",
    "d5 = []\n",
    "y = []\n",
    "for a in sa_index:\n",
    "    for b in range((a[1] - a[0] - win_l + win_i) // win_i):\n",
    "        y.append(2)\n",
    "        data_id += [id_shift] * win_l\n",
    "        d1 += list(data_sa[a[0] + win_i * b : a[0] + win_i * b + win_l, 0])\n",
    "        d2 += list(data_sa[a[0] + win_i * b : a[0] + win_i * b + win_l, 1])\n",
    "        d3 += list(data_sa[a[0] + win_i * b : a[0] + win_i * b + win_l, 2])\n",
    "        d4 += list(data_sa[a[0] + win_i * b : a[0] + win_i * b + win_l, 3])\n",
    "        d5 += list(data_sa[a[0] + win_i * b : a[0] + win_i * b + win_l, 4])        \n",
    "        id_shift += 1\n",
    "#print(len(data_id), len(d1))\n",
    "filt_sa = np.array([d1, d2, d3, d4, d5])\n",
    "\n",
    "data_id = []\n",
    "d1 = []\n",
    "d2 = []\n",
    "d3 = []\n",
    "d4 = []\n",
    "d5 = []\n",
    "y = []    \n",
    "for a in sd_index:\n",
    "    for b in range((a[1] - a[0] - win_l + win_i) // win_i):\n",
    "        y.append(3)\n",
    "        data_id += [id_shift] * win_l\n",
    "        d1 += list(data_sd[a[0] + win_i * b : a[0] + win_i * b + win_l, 0])\n",
    "        d2 += list(data_sd[a[0] + win_i * b : a[0] + win_i * b + win_l, 1])\n",
    "        d3 += list(data_sd[a[0] + win_i * b : a[0] + win_i * b + win_l, 2])\n",
    "        d4 += list(data_sd[a[0] + win_i * b : a[0] + win_i * b + win_l, 3])\n",
    "        d5 += list(data_sd[a[0] + win_i * b : a[0] + win_i * b + win_l, 4])        \n",
    "        id_shift += 1\n",
    "#print(len(data_id), len(d1))\n",
    "filt_sd = np.array([d1, d2, d3, d4, d5])\n",
    "\n",
    "data_id = []\n",
    "d1 = []\n",
    "d2 = []\n",
    "d3 = []\n",
    "d4 = []\n",
    "d5 = []\n",
    "y = []        \n",
    "for a in walk_index[0:100]:\n",
    "    for b in range((a[1] - a[0] - win_l + win_i) // win_i):\n",
    "        y.append(4)\n",
    "        data_id += [id_shift] * win_l\n",
    "        d1 += list(data_walk[a[0] + win_i * b : a[0] + win_i * b + win_l, 0])\n",
    "        d2 += list(data_walk[a[0] + win_i * b : a[0] + win_i * b + win_l, 1])\n",
    "        d3 += list(data_walk[a[0] + win_i * b : a[0] + win_i * b + win_l, 2])\n",
    "        d4 += list(data_walk[a[0] + win_i * b : a[0] + win_i * b + win_l, 3])\n",
    "        d5 += list(data_walk[a[0] + win_i * b : a[0] + win_i * b + win_l, 4])        \n",
    "        id_shift += 1\n",
    "#print(len(data_id), len(d1))\n",
    "#print(id_shift)\n",
    "filt_walk = np.array([d1, d2, d3, d4, d5])\n",
    "\n",
    "data_id = []\n",
    "d1 = []\n",
    "d2 = []\n",
    "d3 = []\n",
    "d4 = []\n",
    "d5 = []\n",
    "y = []        \n",
    "for a in stand_index[0:1]:\n",
    "    for b in range((a[1] - a[0] - win_l + win_i) // win_i):\n",
    "        y.append(5)\n",
    "        data_id += [id_shift] * win_l\n",
    "        d1 += list(data_stand[a[0] + win_i * b : a[0] + win_i * b + win_l, 0])\n",
    "        d2 += list(data_stand[a[0] + win_i * b : a[0] + win_i * b + win_l, 1])\n",
    "        d3 += list(data_stand[a[0] + win_i * b : a[0] + win_i * b + win_l, 2])\n",
    "        d4 += list(data_stand[a[0] + win_i * b : a[0] + win_i * b + win_l, 3])\n",
    "        d5 += list(data_stand[a[0] + win_i * b : a[0] + win_i * b + win_l, 4])        \n",
    "        id_shift += 1\n",
    "#print(len(data_id), len(d1))\n",
    "#print(id_shift)\n",
    "filt_stand = np.array([d1, d2, d3, d4, d5])\n",
    "\n",
    "data_id = []\n",
    "d1 = []\n",
    "d2 = []\n",
    "d3 = []\n",
    "d4 = []\n",
    "d5 = []\n",
    "y = []        \n",
    "for a in sit_index[0:1]:\n",
    "    for b in range((a[1] - a[0] - win_l + win_i) // win_i):\n",
    "        y.append(6)\n",
    "        data_id += [id_shift] * win_l\n",
    "        d1 += list(data_sit[a[0] + win_i * b : a[0] + win_i * b + win_l, 0])\n",
    "        d2 += list(data_sit[a[0] + win_i * b : a[0] + win_i * b + win_l, 1])\n",
    "        d3 += list(data_sit[a[0] + win_i * b : a[0] + win_i * b + win_l, 2])\n",
    "        d4 += list(data_sit[a[0] + win_i * b : a[0] + win_i * b + win_l, 3])\n",
    "        d5 += list(data_sit[a[0] + win_i * b : a[0] + win_i * b + win_l, 4])        \n",
    "        id_shift += 1\n",
    "filt_sit = np.array([d1, d2, d3, d4, d5])\n",
    "        \n",
    "\n",
    "f_sit = get_feature(filt_sit)\n",
    "f_stand = get_feature(filt_stand)\n",
    "f_walk = get_feature(filt_walk)\n",
    "f_stairA = get_feature(filt_sa)\n",
    "f_stairD = get_feature(filt_sd)\n",
    "f_rampA = get_feature(filt_ra)\n",
    "f_rampD = get_feature(filt_rd)\n",
    "\n",
    "f_data = []\n",
    "label = []\n",
    "#    data = []\n",
    "\n",
    "#    l_sit = np.zeros((f_sit.shape[0], 1))\n",
    "#    l_stand = np.zeros((f_stand.shape[0], 1))\n",
    "#    l_walk = np.zeros((f_walk.shape[0], 1))\n",
    "#    l_stairA = np.zeros((f_stairA.shape[0], 1))\n",
    "#    l_stairD = np.zeros((f_stairD.shape[0], 1))\n",
    "#    l_rampA = np.zeros((f_rampA.shape[0], 1))\n",
    "#    l_rampD = np.zeros((f_rampD.shape[0], 1))\n",
    "\n",
    "for i in range(f_sit.shape[0]):\n",
    "    label.append(0)\n",
    "\n",
    "for i in range(f_stand.shape[0]):\n",
    "    label.append(1)\n",
    "\n",
    "for i in range(f_walk.shape[0]):\n",
    "    label.append(2)\n",
    "\n",
    "for i in range(f_stairA.shape[0]):\n",
    "    label.append(3)\n",
    "\n",
    "for i in range(f_stairD.shape[0]):\n",
    "    label.append(4)\n",
    "\n",
    "for i in range(f_rampA.shape[0]):\n",
    "    label.append(5)\n",
    "\n",
    "for i in range(f_rampD.shape[0]):\n",
    "    label.append(6)\n",
    "\n",
    "f_data = np.concatenate((f_sit, f_stand, f_walk, f_stairA, f_stairD, f_rampA, f_rampD), axis = 0)\n",
    "print(f_data.shape)\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 207720)\n"
     ]
    }
   ],
   "source": [
    "print(filt_sit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 22.4min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 38.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed: 59.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed: 83.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed: 111.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3000 out of 3000 | elapsed: 135.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter:  {'C': 80.0, 'gamma': 0.1}\n",
      "best efficience: 0.9947722180731889\n",
      "report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      2055\n",
      "           1       0.00      0.00      0.00       968\n",
      "           2       0.33      0.00      0.00       561\n",
      "           3       0.00      0.00      0.00       757\n",
      "           4       0.00      0.00      0.00       842\n",
      "           5       0.15      1.00      0.25       971\n",
      "           6       0.00      0.00      0.00       541\n",
      "\n",
      "    accuracy                           0.15      6695\n",
      "   macro avg       0.07      0.14      0.04      6695\n",
      "weighted avg       0.05      0.15      0.04      6695\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\install\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#    label = np.concatenate((l_sit, l_stand, l_walk, l_stairA, l_stairD, l_rampA, l_rampD), axis = 0)\n",
    "#    data = np.concatenate((f_data, label), axis = 1)\n",
    "\n",
    "train_data_r, test_data_r, train_label, test_label = train_test_split(f_data, label, test_size = 0.2, random_state = 0)\n",
    "# train_data = preprocessing.normalize(train_data_r, norm = 'max')\n",
    "# test_data = preprocessing.normalize(test_data_r, norm = 'max')\n",
    "scaler = preprocessing.StandardScaler().fit(train_data_r)\n",
    "train_data = scaler.transform(train_data_r)\n",
    "test_data = scaler.transform(test_data_r)\n",
    "'''\n",
    "c = svm.SVC(C = 52, kernel = 'rbf', gamma = 0.1, decision_function_shape = 'ovr')\n",
    "c.fit(train_data, train_label)\n",
    "print(\"train acc:\", c.score(train_data, train_label))\n",
    "print('test acc:', c.score(test_data, test_label))\n",
    "test_pred = c.predict(test_data)\n",
    "print(confusion_matrix(test_label, test_pred))\n",
    "'''\n",
    "parameters = {'gamma':[0.0001, 0.001, 0.01, 0.1, 1, 10], 'C':np.linspace(1, 100, 100)}\n",
    "gs = GridSearchCV(svm.SVC(), parameters, refit = True, cv = 5, verbose = 1, n_jobs = -1)\n",
    "gs.fit(train_data, train_label)\n",
    "print('best parameter: ', gs.best_params_)\n",
    "print('best efficience:', gs.best_score_)\n",
    "print('report: ', classification_report(test_label, gs.predict(test_data_r)))\n",
    "#    knn = KNeighborsClassifier()\n",
    "#    knn.fit(train_data, train_label)\n",
    "#    print(knn.score(train_data, train_label))\n",
    "#    print(knn.score(test_data, test_label))\n",
    "\n",
    "#    test_pred = knn.predict(test_data)\n",
    "#    name = ['sit', 'stand', 'walk', 'stairA', 'stairD', 'rampA', 'rampD']  \n",
    "#    print(confusion_matrix(test_label, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data_r, test_data_r, train_label, test_label = train_test_split(f_data, label, test_size = 0.2, random_state = 0)\n",
    "# train_data = preprocessing.normalize(train_data_r, norm = 'max')\n",
    "# test_data = preprocessing.normalize(test_data_r, norm = 'max')\n",
    "scaler = preprocessing.StandardScaler().fit(train_data_r)\n",
    "train_data = scaler.transform(train_data_r)\n",
    "test_data = scaler.transform(test_data_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.9995519044062733\n",
      "test acc: 0.9955190440627334\n",
      "[[2055    0    0    0    0    0    0]\n",
      " [   0  967    0    0    0    1    0]\n",
      " [   0    0  558    0    0    1    2]\n",
      " [   0    0    0  752    3    2    0]\n",
      " [   0    0    3    4  834    1    0]\n",
      " [   0    0    0    0    0  968    3]\n",
      " [   0    0    1    0    1    8  531]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "c = svm.SVC(C = 80, kernel = 'rbf', gamma = 0.1, decision_function_shape = 'ovr')# \n",
    "c.fit(train_data, train_label)\n",
    "print(\"train acc:\", c.score(train_data, train_label))\n",
    "print('test acc:', c.score(test_data, test_label))\n",
    "test_pred = c.predict(test_data)\n",
    "print(confusion_matrix(test_label, test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
